{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192d403",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 生成器，判别器未采用残差\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from lib.utils import dummy_context_mgr\n",
    "\n",
    "\n",
    "class CLIP_IMG_ENCODER(nn.Module):\n",
    "    def __init__(self, CLIP):\n",
    "        super(CLIP_IMG_ENCODER, self).__init__()\n",
    "        model = CLIP.visual\n",
    "        # print(model)\n",
    "        self.define_module(model)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.conv1 = model.conv1\n",
    "        self.class_embedding = model.class_embedding\n",
    "        self.positional_embedding = model.positional_embedding\n",
    "        self.ln_pre = model.ln_pre\n",
    "        self.transformer = model.transformer\n",
    "        self.ln_post = model.ln_post\n",
    "        self.proj = model.proj\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.conv1.weight.dtype\n",
    "\n",
    "    def transf_to_CLIP_input(self,inputs):\n",
    "        device = inputs.device\n",
    "        if len(inputs.size()) != 4:\n",
    "            raise ValueError('Expect the (B, C, X, Y) tensor.')\n",
    "        else:\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\\\n",
    "                .unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "            var = torch.tensor([0.26862954, 0.26130258, 0.27577711])\\\n",
    "                .unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "            inputs = F.interpolate(inputs*0.5+0.5, size=(224, 224))\n",
    "            inputs = ((inputs+1)*0.5-mean)/var\n",
    "            return inputs\n",
    "\n",
    "    def forward(self, img: torch.Tensor):\n",
    "        x = self.transf_to_CLIP_input(img)\n",
    "        x = x.type(self.dtype)\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        grid =  x.size(-1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "        # NLD -> LND\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # Local features\n",
    "        #selected = [1,4,7,12]\n",
    "        selected = [1,4,8]\n",
    "        local_features = []\n",
    "        for i in range(12):\n",
    "            x = self.transformer.resblocks[i](x)\n",
    "            if i in selected:\n",
    "                local_features.append(x.permute(1, 0, 2)[:, 1:, :].permute(0, 2, 1).reshape(-1, 768, grid, grid).contiguous().type(img.dtype))\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "        return torch.stack(local_features, dim=1), x.type(img.dtype)\n",
    "\n",
    "\n",
    "class CLIP_TXT_ENCODER(nn.Module):\n",
    "    def __init__(self, CLIP):\n",
    "        super(CLIP_TXT_ENCODER, self).__init__()\n",
    "        self.define_module(CLIP)\n",
    "        # print(model)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def define_module(self, CLIP):\n",
    "        self.transformer = CLIP.transformer\n",
    "        self.vocab_size = CLIP.vocab_size\n",
    "        self.token_embedding = CLIP.token_embedding\n",
    "        self.positional_embedding = CLIP.positional_embedding\n",
    "        self.ln_final = CLIP.ln_final\n",
    "        self.text_projection = CLIP.text_projection\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.transformer.resblocks[0].mlp.c_fc.weight.dtype\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        sent_emb = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return sent_emb, x\n",
    "\n",
    "\n",
    "class CLIP_Mapper(nn.Module):\n",
    "    def __init__(self, CLIP):\n",
    "        super(CLIP_Mapper, self).__init__()\n",
    "        model = CLIP.visual\n",
    "        # print(model)\n",
    "        self.define_module(model)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.conv1 = model.conv1\n",
    "        self.class_embedding = model.class_embedding\n",
    "        self.positional_embedding = model.positional_embedding\n",
    "        self.ln_pre = model.ln_pre\n",
    "        self.transformer = model.transformer\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.conv1.weight.dtype\n",
    "\n",
    "    def forward(self, img: torch.Tensor, prompts: torch.Tensor):\n",
    "        x = img.type(self.dtype)\n",
    "        prompts = prompts.type(self.dtype)\n",
    "        grid = x.size(-1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  \n",
    "        # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "        # NLD -> LND\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # Local features\n",
    "        selected = [1,2,3,4,5,6,7,8]\n",
    "        begin, end = 0, 12\n",
    "        prompt_idx = 0\n",
    "        for i in range(begin, end):\n",
    "            if i in selected:\n",
    "                prompt = prompts[:,prompt_idx,:].unsqueeze(0)\n",
    "                prompt_idx = prompt_idx+1\n",
    "                x = torch.cat((x,prompt), dim=0)\n",
    "                x = self.transformer.resblocks[i](x)\n",
    "                x = x[:-1,:,:]\n",
    "            else:\n",
    "                x = self.transformer.resblocks[i](x)\n",
    "        return x.permute(1, 0, 2)[:, 1:, :].permute(0, 2, 1).reshape(-1, 768, grid, grid).contiguous().type(img.dtype)\n",
    "\n",
    "\n",
    "class CLIP_Adapter(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, CLIP):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        self.FBlocks = nn.ModuleList([])\n",
    "        self.FBlocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for i in range(map_num-1):\n",
    "            self.FBlocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        self.conv_fuse = nn.Conv2d(out_ch, CLIP_ch, 5, 1, 2)\n",
    "        self.CLIP_ViT = CLIP_Mapper(CLIP)\n",
    "        self.conv = nn.Conv2d(768, G_ch, 5, 1, 2)\n",
    "        #\n",
    "        self.fc_prompt = nn.Linear(cond_dim, CLIP_ch*8)\n",
    "\n",
    "    def forward(self,out,c):\n",
    "        prompts = self.fc_prompt(c).view(c.size(0),-1,self.CLIP_ch)\n",
    "        for FBlock in self.FBlocks:\n",
    "            out = FBlock(out,c)\n",
    "        fuse_feat = self.conv_fuse(out)\n",
    "        map_feat = self.CLIP_ViT(fuse_feat,prompts)\n",
    "        return self.conv(fuse_feat+0.1*map_feat)\n",
    "\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.mixed_precision = mixed_precision\n",
    "        # build CLIP Mapper\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.CLIP_ch = 768\n",
    "        self.fc_code = nn.Linear(nz, self.code_sz*self.code_sz*self.code_ch)\n",
    "        self.mapping = CLIP_Adapter(self.code_ch, self.mid_ch, self.code_ch, ngf*8, self.CLIP_ch, cond_dim+nz, 3, 1, 1, 4, CLIP)\n",
    "        # build GBlocks\n",
    "        self.GBlocks = nn.ModuleList([])\n",
    "        in_out_pairs = list(get_G_in_out_chs(ngf, imsize))\n",
    "        imsize = 4\n",
    "        for idx, (in_ch, out_ch) in enumerate(in_out_pairs):\n",
    "            if idx<(len(in_out_pairs)-1):\n",
    "                imsize = imsize*2\n",
    "            else:\n",
    "                imsize = 224\n",
    "            self.GBlocks.append(G_Block(cond_dim+nz, in_ch, out_ch, imsize))\n",
    "        # to RGB image\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Conv2d(out_ch, ch_size, 3, 1, 1),\n",
    "            #nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise, c, eval=False): # x=noise, c=ent_emb\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "            cond = torch.cat((noise, c), dim=1)\n",
    "            out = self.mapping(self.fc_code(noise).view(noise.size(0), self.code_ch, self.code_sz, self.code_sz), cond)\n",
    "            # fuse text and visual features\n",
    "            for GBlock in self.GBlocks:\n",
    "                out = GBlock(out, cond)\n",
    "            # convert to RGB image\n",
    "            out = self.to_rgb(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 定义鉴别器网络D\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, ndf, imsize, ch_size, mixed_precision):\n",
    "        super(NetD, self).__init__()\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.DBlocks = nn.ModuleList([\n",
    "            D_Block(768, 768, 3, 1, 1, res=True, CLIP_feat=True),\n",
    "            D_Block(768, 768, 3, 1, 1, res=True, CLIP_feat=True),\n",
    "        ])\n",
    "        self.main = D_Block(768, 512, 3, 1, 1, res=True, CLIP_feat=False)\n",
    "\n",
    "    def forward(self, h):\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision else dummy_context_mgr() as mpc:\n",
    "            out = h[:,0]\n",
    "            for idx in range(len(self.DBlocks)):\n",
    "                out = self.DBlocks[idx](out, h[:,idx+1])\n",
    "            out = self.main(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class NetC(nn.Module):\n",
    "    def __init__(self, ndf, cond_dim, mixed_precision):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.joint_conv = nn.Sequential(\n",
    "            nn.Conv2d(512+512, 128, 4, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, out, cond):\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision else dummy_context_mgr() as mpc:\n",
    "            cond = cond.view(-1, self.cond_dim, 1, 1)\n",
    "            cond = cond.repeat(1, 1, 7, 7)\n",
    "            h_c_code = torch.cat((out, cond), 1)\n",
    "            out = self.joint_conv(h_c_code)\n",
    "        return out\n",
    "\n",
    "\n",
    "class M_Block(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "        super(M_Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, k, s, p)\n",
    "        self.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, k, s, p)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, h, text):\n",
    "        h = self.conv1(h)\n",
    "        h = self.fuse1(h, text)\n",
    "        h = self.conv2(h)\n",
    "        h = self.fuse2(h, text)\n",
    "        return h\n",
    "\n",
    "    def forward(self, h, c):\n",
    "        return self.shortcut(h) + self.residual(h, c)\n",
    "\n",
    "\n",
    "class G_Block(nn.Module):\n",
    "    def __init__(self, cond_dim, in_ch, out_ch, imsize):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.learnable_sc = in_ch != out_ch \n",
    "        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, h, y):\n",
    "        h = self.fuse1(h, y)\n",
    "        h = self.c1(h)\n",
    "        h = self.fuse2(h, y)\n",
    "        h = self.c2(h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, h, y):\n",
    "        h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        return self.shortcut(h) + self.residual(h, y)\n",
    "\n",
    "\n",
    "class D_Block(nn.Module):\n",
    "    def __init__(self, fin, fout, k, s, p, res, CLIP_feat):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.res, self.CLIP_feat = res, CLIP_feat\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        self.conv_r = nn.Sequential(\n",
    "            nn.Conv2d(fin, fout, k, s, p, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(fout, fout, k, s, p, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        self.conv_s = nn.Conv2d(fin, fout, 1, stride=1, padding=0)\n",
    "        if self.res==True:\n",
    "            self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        if self.CLIP_feat==True:\n",
    "            self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, CLIP_feat=None):\n",
    "        res = self.conv_r(x)\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        if (self.res==True)and(self.CLIP_feat==True):\n",
    "            return x + self.gamma*res + self.beta*CLIP_feat\n",
    "        elif (self.res==True)and(self.CLIP_feat!=True):\n",
    "            return x + self.gamma*res\n",
    "        elif (self.res!=True)and(self.CLIP_feat==True):\n",
    "            return x + self.beta*CLIP_feat\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class DFBLK(nn.Module):\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.affine0(x, y)\n",
    "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
    "        h = self.affine1(h, y)\n",
    "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "\n",
    "        self.fc_gamma = nn.Sequential(OrderedDict([\n",
    "            ('linear1',nn.Linear(cond_dim, num_features)),\n",
    "            ('relu1',nn.ReLU(inplace=True)),\n",
    "            ('linear2',nn.Linear(num_features, num_features)),\n",
    "            ]))\n",
    "        self.fc_beta = nn.Sequential(OrderedDict([\n",
    "            ('linear1',nn.Linear(cond_dim, num_features)),\n",
    "            ('relu1',nn.ReLU(inplace=True)),\n",
    "            ('linear2',nn.Linear(num_features, num_features)),\n",
    "            ]))\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n",
    "        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n",
    "        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n",
    "        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        weight = self.fc_gamma(y)\n",
    "        bias = self.fc_beta(y)        \n",
    "\n",
    "        if weight.dim() == 1:\n",
    "            weight = weight.unsqueeze(0)\n",
    "        if bias.dim() == 1:\n",
    "            bias = bias.unsqueeze(0)\n",
    "\n",
    "        size = x.size()\n",
    "        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        return weight * x + bias\n",
    "\n",
    "\n",
    "def get_G_in_out_chs(nf, imsize):\n",
    "    layer_num = int(np.log2(imsize))-1\n",
    "    channel_nums = [nf*min(2**idx, 8) for idx in range(layer_num)]\n",
    "    channel_nums = channel_nums[::-1]\n",
    "    in_out_pairs = zip(channel_nums[:-1], channel_nums[1:])\n",
    "    return in_out_pairs\n",
    "\n",
    "\n",
    "def get_D_in_out_chs(nf, imsize):\n",
    "    layer_num = int(np.log2(imsize))-1\n",
    "    channel_nums = [nf*min(2**idx, 8) for idx in range(layer_num)]\n",
    "    in_out_pairs = zip(channel_nums[:-1], channel_nums[1:])\n",
    "    return in_out_pairs\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
